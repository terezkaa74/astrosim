# Offline PDF Reader with Local LLM

A Windows desktop application that uses a locally bundled Large Language Model to understand and analyze PDF documents. All processing happens on your machine with no internet connection required.

## Features

- **True AI Understanding** - Uses local LLM (Phi-2, Phi-3, Mistral) for real language comprehension
- **100% Offline** - Works completely offline, no internet required after setup
- **No APIs** - No OpenAI, Anthropic, or any cloud services
- **Windows Desktop App** - Native .exe application built with Electron
- **PDF Processing** - Extracts text, detects structure (title, abstract, sections), and identifies tables
- **AI-Powered Q&A** - Ask questions and get intelligent answers generated by the LLM
- **AI Summaries** - Generates coherent summaries using the local LLM
- **Export Options** - Export summaries as .txt, .md, or tables as .csv
- **CPU-Only** - Runs on any Windows machine without GPU

## Quick Start

See [QUICKSTART.md](QUICKSTART.md) for a 5-minute setup guide.

### For End Users

1. Download and run the Windows installer (.exe)
2. Launch the application
3. Wait for "LLM Ready" status (green indicator in footer)
4. Upload a research PDF
5. View the extracted document structure
6. Read the AI-generated summary
7. Ask questions and chat with the document
8. Export summaries or tables

### For Developers

#### Development Mode

**Prerequisites:**
- Node.js 18+
- Python 3.10 or 3.11
- Windows 10/11

**Setup:**

```bash
npm install
cd python-backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
python download_model.py  # Download a model
```

**Run:**

Terminal 1:
```bash
cd python-backend
venv\Scripts\activate
python main.py
```

Terminal 2:
```bash
npm run electron:dev
```

#### Build for Distribution

```bash
build-windows.bat
```

See [SETUP_GUIDE.md](SETUP_GUIDE.md) for detailed build instructions.

## How It Works

### Architecture

1. **Electron Desktop Wrapper**
   - Provides native Windows application experience
   - Manages Python backend process lifecycle
   - Handles file system access for models

2. **Python Backend (FastAPI + llama.cpp)**
   - Runs local HTTP server on localhost:8000
   - Loads GGUF model into memory
   - Handles PDF text extraction
   - Processes LLM inference requests

3. **React Frontend**
   - User interface built with React + Vite
   - Communicates with backend via HTTP
   - Displays document structure and LLM responses

4. **Local LLM (llama.cpp)**
   - CPU-only inference using quantized models
   - Supports GGUF format (Phi-2, Phi-3, Mistral, etc.)
   - 4-bit quantization for efficient memory usage
   - No GPU required

### PDF Processing

1. PDF uploaded through Electron UI
2. Sent to Python backend via HTTP
3. PyPDF2 extracts raw text
4. React frontend detects structure using patterns
5. LLM generates summary and answers questions

### Question Answering

Unlike traditional keyword matching, this uses a real LLM:

1. User asks a question
2. Document text is used as context
3. LLM receives prompt with question + context
4. LLM generates intelligent, contextual answer
5. Response displayed in chat interface

### Summary Generation

1. Document text extracted from PDF
2. First 8000 characters sent to LLM as context
3. LLM generates coherent summary covering:
   - Main topic and purpose
   - Key findings
   - Conclusions
4. Summary displayed in dedicated tab

## Technology Stack

- **Electron** - Desktop application wrapper
- **React** - UI framework
- **Vite** - Build tool and bundler
- **Python + FastAPI** - Backend API server
- **llama-cpp-python** - LLM inference engine
- **PyPDF2** - PDF text extraction
- **PyInstaller** - Python to .exe compiler

## Available Models

- **TinyLlama 1.1B** (669 MB) - Fastest, basic quality
- **Phi-2 2.7B** (1.6 GB) - Good balance (recommended)
- **Phi-3 Mini 3.8B** (2.2 GB) - Better quality
- **Mistral 7B** (4.4 GB) - Best quality, slower

All models are quantized (Q4_K_M) for efficiency.

## Advantages Over Classical NLP

**Previous Version (TF-IDF):**
- Keyword matching
- No understanding of context
- Extracted sentences, didn't generate text
- Simple but limited

**This Version (Local LLM):**
- True language comprehension
- Understands context and relationships
- Generates new, coherent text
- Much better quality answers
- Still 100% offline

## Limitations

- Requires 4-8GB disk space for model
- Slower than cloud LLMs (5-30 seconds per response)
- Quality depends on model size
- CPU-only may be slow on older machines
- Windows only (can be adapted for Mac/Linux)

## Privacy & Security

- All processing happens on your local machine
- No data leaves your device
- No tracking or analytics
- No external API calls
- No internet connection required after setup
- Your PDFs are never uploaded anywhere
- Model runs entirely on your CPU

## System Requirements

**Minimum:**
- Windows 10
- 4GB RAM
- 8GB free disk space
- Dual-core CPU

**Recommended:**
- Windows 10/11
- 8GB+ RAM
- 10GB+ free disk space
- Quad-core CPU or better

## Frequently Asked Questions

**Q: Does this need internet?**
A: Only for initial setup (downloading the model). After that, 100% offline.

**Q: Is it as good as ChatGPT?**
A: No, but it's free, private, and works offline. Quality depends on model size.

**Q: Can I use my own models?**
A: Yes! Place any GGUF format model in the `models` directory.

**Q: Does it work on Mac/Linux?**
A: The code is cross-platform but this build is Windows-only. You can modify `electron/main.cjs` for Mac/Linux.

**Q: How long do responses take?**
A: Depends on hardware and model. Typically 5-30 seconds per response on modern CPUs.

**Q: Can I use GPU acceleration?**
A: The default build is CPU-only. You can recompile llama-cpp-python with CUDA support for GPU acceleration.

## Contributing

Contributions welcome! See [SETUP_GUIDE.md](SETUP_GUIDE.md) for development setup.

## License

MIT

## Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) - LLM inference engine
- [Hugging Face](https://huggingface.co/) - Model hosting
- Model creators: Microsoft (Phi), Mistral AI (Mistral)
- PDF.js team
- Electron team
